import numpy as np  # numpyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’npã¨ã„ã†ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ•°å€¤è¨ˆç®—ç”¨ï¼‰
import matplotlib.pyplot as plt  # ã‚°ãƒ©ãƒ•æç”»ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªmatplotlibã®pyplotãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’pltã¨ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from functions import sigmoid, softmax, relu, cross_entropy_error  # å¿…è¦ãªæ´»æ€§åŒ–é–¢æ•°ã¨æå¤±é–¢æ•°ã‚’functionsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from optimizer import SGD, Momentum, Nesterov, AdaGrad, RMSprop, Adam  # åŒä¸€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã®optimizerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰æœ€é©åŒ–æ‰‹æ³•ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import time  # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬ç”¨

# ä»¥ä¸‹ã®è¨­å®šã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’AppleGothicã«å¤‰æ›´ï¼ˆmacOSã®å ´åˆï¼‰
import matplotlib
matplotlib.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False


def load_mnist():  # MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—ã—å‰å‡¦ç†ã‚’è¡Œã†é–¢æ•°
    """deep-learning-from-scratchã®mnist.pyã‚’ä½¿ç”¨ã—ã¦MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—ã—ã€å‰å‡¦ç†ã‚’è¡Œã†é–¢æ•°
    normalize=True : ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’0ï½1ã«æ­£è¦åŒ–
    flatten=True   : ç”»åƒã‚’ä¸€æ¬¡å…ƒé…åˆ—ã«å¤‰æ›
    one_hot_label=True : ãƒ©ãƒ™ãƒ«ã‚’One-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹
    """
    from mnist import load_mnist as ds_load_mnist  # mnist.pyã‹ã‚‰load_mnisté–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    (X_train, T_train), (X_test, T_test) = ds_load_mnist(normalize=True, flatten=True, one_hot_label=True)  # MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ã€è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
    return X_train, X_test, T_train, T_test


def one_hot(labels, num_classes=10):  # ãƒ©ãƒ™ãƒ«ã‚’one-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã®å®šç¾©
    one_hot_labels = np.zeros((labels.shape[0], num_classes))  # ãƒ©ãƒ™ãƒ«æ•°Ã—ã‚¯ãƒ©ã‚¹æ•°ã®ã‚¼ãƒ­è¡Œåˆ—ã‚’ä½œæˆ
    for idx, label in enumerate(labels):  # å„ãƒ©ãƒ™ãƒ«ã«å¯¾ã—ã¦ãƒ«ãƒ¼ãƒ—
        one_hot_labels[idx, int(label)] = 1  # è©²å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹ã®ä½ç½®ã«1ã‚’ã‚»ãƒƒãƒˆ
    return one_hot_labels  # one-hotè¡¨ç¾ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’è¿”ã™


class MultiLayerNet:  # å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰ã‚’å®Ÿç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹ã®å®šç¾©
    def __init__(self, input_size=784, hidden_dims=[50], output_size=10,
                 hidden_activation=sigmoid):  # ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿: å…¥åŠ›ã‚µã‚¤ã‚ºã€ä¸­é–“å±¤ã®æ§‹é€ ã€å‡ºåŠ›ã‚µã‚¤ã‚ºã‚’æŒ‡å®š
        self.layers = [input_size] + hidden_dims + [output_size]  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å„å±¤ã®ã‚µã‚¤ã‚ºã‚’ãƒªã‚¹ãƒˆã§å®šç¾©ï¼ˆå…¥åŠ›å±¤â†’ä¸­é–“å±¤â†’å‡ºåŠ›å±¤ï¼‰
        self.hidden_activation = hidden_activation
        self.output_activation = softmax  # å‡ºåŠ›å±¤ã¯å¸¸ã«softmaxã§å›ºå®š
        self.num_layers = len(self.layers) - 1  # é‡ã¿ã‚’æŒã¤ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ•°ï¼ˆä¸­é–“å±¤ã¨å‡ºåŠ›å±¤ã®åˆè¨ˆï¼‰ã‚’è¨ˆç®—
        self.params = {}  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ï¼‰ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
        for i in range(self.num_layers):  # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å¯¾ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
            self.params['W' + str(i+1)] = np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(2.0 / self.layers[i])  # HeåˆæœŸåŒ–ã‚’ç”¨ã„ã¦é‡ã¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®š
            self.params['b' + str(i+1)] = np.zeros(self.layers[i+1])  # ãƒã‚¤ã‚¢ã‚¹ã¯ã‚¼ãƒ­ã§åˆæœŸåŒ–
    
    def predict(self, x):  # é †ä¼æ’­ã«ã‚ˆã‚Šäºˆæ¸¬çµæœã‚’è¨ˆç®—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        out = x  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã®åˆæœŸå€¤ã«è¨­å®š
        for i in range(1, self.num_layers):  # ä¸­é–“å±¤ã¾ã§ãƒ«ãƒ¼ãƒ—
            W = self.params['W' + str(i)]  # iå±¤ç›®ã®é‡ã¿ã‚’å–å¾—
            b = self.params['b' + str(i)]  # iå±¤ç›®ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å–å¾—
            out = self.hidden_activation(np.dot(out, W) + b)  # ç·šå½¢å¤‰æ›ã«hidden_activationã‚’é©ç”¨
        W = self.params['W' + str(self.num_layers)]  # å‡ºåŠ›å±¤ã®é‡ã¿ã‚’å–å¾—
        b = self.params['b' + str(self.num_layers)]  # å‡ºåŠ›å±¤ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å–å¾—
        out = self.output_activation(np.dot(out, W) + b)  # å‡ºåŠ›å±¤ã§ç·šå½¢å¤‰æ›ã«output_activationã‚’é©ç”¨ã—ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
        return out  # äºˆæ¸¬çµæœï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰ã‚’è¿”ã™
    
    def loss(self, x, t):  # æå¤±é–¢æ•°ã®å€¤ã‚’è¨ˆç®—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        y = self.predict(x)  # äºˆæ¸¬çµæœã‚’è¨ˆç®—
        return cross_entropy_error(y, t)  # äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®ã‚’è¨ˆç®—ã—ã¦è¿”ã™
    
    def accuracy(self, x, t):  # äºˆæ¸¬ç²¾åº¦ï¼ˆæ­£è§£ç‡ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        y = self.predict(x)  # äºˆæ¸¬çµæœã‚’è¨ˆç®—
        y_pred = np.argmax(y, axis=1)  # äºˆæ¸¬çµæœã‹ã‚‰æœ€å¤§ç¢ºç‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆäºˆæ¸¬ãƒ©ãƒ™ãƒ«ï¼‰
        t_label = np.argmax(t, axis=1)  # æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆone-hotã®å ´åˆï¼‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        return np.sum(y_pred == t_label) / float(x.shape[0])  # æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã®ä¸€è‡´ç‡ã‚’è¨ˆç®—ã—ã¦è¿”ã™
    
    def gradient(self, x, t):  # èª¤å·®é€†ä¼æ’­æ³•ã«ã‚ˆã‚Šå„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        grads = {}  # å‹¾é…ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
        batch_num = x.shape[0]  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—
        activations = [x]  # å„å±¤ã®å‡ºåŠ›ï¼ˆæ´»æ€§åŒ–å€¤ï¼‰ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆã€‚åˆæœŸå€¤ã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        pre_activations = []  # å„å±¤ã®ç·šå½¢å¤‰æ›å¾Œã®å€¤ï¼ˆæ´»æ€§åŒ–å‰ï¼‰ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        for i in range(1, self.num_layers + 1):  # å„å±¤ã«å¯¾ã—ã¦é †ä¼æ’­ã‚’å®Ÿæ–½
            W = self.params['W' + str(i)]  # iå±¤ç›®ã®é‡ã¿ã‚’å–å¾—
            b = self.params['b' + str(i)]  # iå±¤ç›®ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å–å¾—
            a = np.dot(activations[i-1], W) + b  # ç·šå½¢å¤‰æ›ã‚’è¨ˆç®—
            pre_activations.append(a)  # ç·šå½¢å¤‰æ›çµæœã‚’ä¿å­˜
            if i == self.num_layers:  # æœ€çµ‚å±¤ã®å ´åˆ
                z = self.output_activation(a)  # output_activationã‚’é©ç”¨
            else:  # ä¸­é–“å±¤ã®å ´åˆ
                z = self.hidden_activation(a)  # hidden_activationã‚’é©ç”¨
            activations.append(z)  # æ´»æ€§åŒ–çµæœã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        delta = (activations[-1] - t) / batch_num  # å‡ºåŠ›å±¤ã®èª¤å·®ï¼ˆäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹å¾®åˆ†ï¼‰ã‚’è¨ˆç®—
        grads['W' + str(self.num_layers)] = np.dot(activations[-2].T, delta)  # å‡ºåŠ›å±¤ã®é‡ã¿ã®å‹¾é…ã‚’è¨ˆç®—
        grads['b' + str(self.num_layers)] = np.sum(delta, axis=0)  # å‡ºåŠ›å±¤ã®ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…ã‚’è¨ˆç®—
        for i in range(self.num_layers - 1, 0, -1):  # é€†ä¼æ’­ã‚’ä¸­é–“å±¤ã«å‘ã‹ã£ã¦å®Ÿæ–½
            W_next = self.params['W' + str(i+1)]  # æ¬¡ã®å±¤ã®é‡ã¿ã‚’å–å¾—
            if self.hidden_activation == sigmoid:
                derivative = activations[i] * (1 - activations[i])
            else:
                derivative = (pre_activations[i-1] > 0).astype(float)
            delta = np.dot(delta, W_next.T) * derivative  # èª¤å·®ã‚’é€†ä¼æ’­ã—ã€æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ†ã§é‡ã¿ä»˜ã‘
            grads['W' + str(i)] = np.dot(activations[i-1].T, delta)  # iå±¤ç›®ã®é‡ã¿ã®å‹¾é…ã‚’è¨ˆç®—
            grads['b' + str(i)] = np.sum(delta, axis=0)  # iå±¤ç›®ã®ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…ã‚’è¨ˆç®—
        return grads  # ã™ã¹ã¦ã®å±¤ã®å‹¾é…ã‚’è¿”ã™


if __name__ == '__main__':  # è‡ªå‹•å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰
    experiments = [
        # ğŸŸ¢ æ±åŒ–ã§ãã¦ã„ã‚‹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        {"hidden_dims": [64], "epochs": 50, "batch_size": 128, "lr": 0.01},
        {"hidden_dims": [128, 128], "epochs": 50, "batch_size": 128, "lr": 0.01},
        {"hidden_dims": [256, 256, 256], "epochs": 100, "batch_size": 128, "lr": 0.005},

        # ğŸŸ¡ è»½åº¦ã€œä¸­åº¦ã®éå­¦ç¿’
        {"hidden_dims": [512, 512, 512, 512], "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [1024, 1024], "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [1024]*6, "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [64]*20, "epochs": 100, "batch_size": 128, "lr": 0.01},
        {"hidden_dims": [8]*12, "epochs": 100, "batch_size": 128, "lr": 0.01},

        # ğŸ”´ é‡åº¦ã®éå­¦ç¿’
        {"hidden_dims": [1024]*10, "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [2048, 1024, 512, 256], "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [512]*30, "epochs": 100, "batch_size": 128, "lr": 0.001},
        {"hidden_dims": [512]*6, "epochs": 300, "batch_size": 128, "lr": 0.0001},

        # âš« ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚‹éå­¦ç¿’
        {"hidden_dims": [512, 512], "epochs": 100, "batch_size": 128, "lr": 0.001, "train_ratio": 0.25},
        {"hidden_dims": [1024, 1024], "epochs": 100, "batch_size": 128, "lr": 0.001, "train_ratio": 0.10},
        {"hidden_dims": [1024, 1024, 512], "epochs": 100, "batch_size": 128, "lr": 0.001, "train_ratio": 0.017}
    ]

    for idx, config in enumerate(experiments, 1):
        print(f"\n===== Running Experiment A{idx}: {config} =====")
        X_train, X_test, T_train, T_test = load_mnist()
        train_ratio = config.get("train_ratio", 1.0)
        if train_ratio < 1.0:
            N = int(len(X_train) * train_ratio)
            X_train, T_train = X_train[:N], T_train[:N]

        net = MultiLayerNet(
            input_size=784,
            hidden_dims=config["hidden_dims"],
            output_size=10,
            hidden_activation=relu
        )
        optimizer = Adam(lr=config["lr"])
        train_acc_list, test_acc_list = [], []
        iter_per_epoch = max(1, X_train.shape[0] // config["batch_size"])
        start_time = time.time()

        for epoch in range(config["epochs"]):
            for _ in range(iter_per_epoch):
                batch_mask = np.random.choice(X_train.shape[0], config["batch_size"])
                x_batch = X_train[batch_mask]
                t_batch = T_train[batch_mask]
                grads = net.gradient(x_batch, t_batch)
                optimizer.update(net.params, grads)
            train_acc = net.accuracy(X_train, T_train)
            test_acc = net.accuracy(X_test, T_test)
            train_acc_list.append(train_acc)
            test_acc_list.append(test_acc)
            print(f"Epoch {epoch+1}/{config['epochs']} - Train: {train_acc:.4f}, Test: {test_acc:.4f}")

        layer_str = '-'.join(str(n) for n in config["hidden_dims"])
        filename = (
            f"layers[{layer_str}]_ep{config['epochs']}_bs{config['batch_size']}"
            f"_lr{config['lr']}_Adam_mid-relu_out-softmax.png"
        )
        title = (
            f"Layers:{len(config['hidden_dims'])}({layer_str}) bs:{config['batch_size']} lr:{config['lr']} "
            f"opt:Adam hid_act:relu out_act:softmax"
        )
        elapsed = time.time() - start_time

        epochs_range = np.arange(1, config["epochs"]+1)
        plt.figure()
        plt.plot(epochs_range, train_acc_list, label='Training Accuracy')
        plt.plot(epochs_range, test_acc_list, label='Test Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title(title)
        plt.legend()
        plt.grid(True)
        plt.figtext(0.01, 0.02, f"Execution time: {elapsed:.2f} s", ha='left', va='bottom')
        plt.savefig(filename)
        print(f"âœ… Saved: {filename} ({elapsed:.2f}ç§’)")
        plt.close()

    print("âœ… All experiments completed")
